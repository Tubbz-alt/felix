@title Synchronous Scheduler
@tangler flx_sync.hpp         = share/lib/rtl/flx_sync.hpp
@tangler flx_sync.cpp         = share/src/rtl/flx_sync.cpp

@h1 Synchronous Support System
This class encapsulate the core Felix synchronous scheduling
mechanism and services synchronous service calls.

The scheduler method @{frun} executes @{fthread_t} fibres 
from the scheduler queue @{active}, performing
synchronous service calls made by the fibres until it
is unable to proceed.

It then suspends and returns a code indication one of two conditions.
Either the scheduler is @{blocked} because there are no more
active fibres on the queue to resume, or, it has received a
non-synchronous service request it is unable to satisfy,
in which case it returns @{delegated} indicating it is delegating
the responsibility to satisfy the service request to its caller.

The variable @{request} contains the service call which the
scheduler is delegating.

The scheduler itself is a finite state machine with three states:
it is ready to resume the current fibre, it is ready to get
the next fibre from the queue, or it is blocked because the
current fibre has gone and the queue is empty.

Synchronous reads and writes can suspend or activate fibres.
The special external multiwrite provides a way to populate
the scheduler queue externally by pushing waiting fibres
off a synchronous channel into the active queue.


@tangle flx_sync.hpp

#ifndef __FLX_SYNC_H__
#define __FLX_SYNC_H__

#include "flx_gc.hpp"
#include "flx_rtl.hpp"
#include <list>
#include <atomic>
#include "flx_async.hpp"
#include "pthread_thread.hpp"

namespace flx { namespace run {

// *************************************
// fthread_list has grown to include the async control object
// and its ready list
//
// this object contains the data shared by multiple pthreads
// pooled to run coroutines concurrently
// *************************************

struct RTL_EXTERN fthread_list {
  ::std::list<::flx::rtl::fthread_t*> fthreads;
  fthread_list(fthread_list const&) = delete;
  fthread_list& operator=(fthread_list const&) = delete;
public:

  ::std::atomic<int> thread_count; // n threads sharing list
  ::std::mutex active_lock;
  ::std::mutex *pactive_lock;

  size_t async_count; // pending async jobs
  async_hooker* async; // async dispatch and ready list object

  fthread_list ();
  ~fthread_list ();

  void push_back(::flx::rtl::fthread_t *);
  void push_front(::flx::rtl::fthread_t *);
  size_t size() const;
  ::flx::rtl::fthread_t *front()const;
  ::flx::rtl::fthread_t *pop_front();
  ::std::list<::flx::rtl::fthread_t*>::iterator begin();
  ::std::list<::flx::rtl::fthread_t*>::iterator end();
};


// This class handles synchronous channel I/O and fthreads
struct RTL_EXTERN sync_sched {
  sync_sched () = delete;
  sync_sched (sync_sched const&) = delete;
  sync_sched &operator=(sync_sched const&) = delete;

  bool debug_driver;

  // the garbage collector and general control object
  ::flx::gc::generic::collector_t *collector;

  // scheduler queue
  fthread_list *active;

  // temporary for currently running fibre
  ::flx::rtl::fthread_t *ft;

  // variable to hold service request
  ::flx::rtl::_uctor_ *request;

  // type for the state of the scheduler
  // when it suspends by returning.
  enum fstate_t { blocked, delegated };

  // debugging helper to get a description of
  // the suspended scheduler state
  static char const * get_fstate_desc(fstate_t);

  // debugging helper to get a description of
  // the running scheduler state
  char const * get_fpc_desc();

  sync_sched (
    bool debug_driver_,
    ::flx::gc::generic::gc_profile_t *gcp_,
    fthread_list *active_
  );

private:
  // helper routines.
  void impl_push_new(::flx::rtl::fthread_t*);
  void impl_push_old(::flx::rtl::fthread_t*);

public:
  void push_new(::flx::rtl::fthread_t*);
  void push_old(::flx::rtl::fthread_t*);
  fstate_t frun();

  // a special routine to allow a multiwrite to be performed
  // from outside the scheduler whilst it is suspended.
  void external_multi_swrite(::flx::rtl::schannel_t*, void*);
protected:
  // handlers for synchronous service calls.
  void do_yield();
  void do_spawn_fthread();
  void do_schedule_fthread();
  void do_sread();
  void do_swrite();
  void do_multi_swrite();
  void do_kill();
  void show_state();
};

}}

#endif
@

@tangle flx_sync.cpp

#include <stdio.h>

#include "flx_sync.hpp"

using namespace flx::rtl;

namespace flx { namespace run {

// ***************************************************
// fthread_list
// ***************************************************
fthread_list::fthread_list() : 
  thread_count(1),
  async_count(0),
  async(nullptr),
  pactive_lock(nullptr)
{}


fthread_t *fthread_list::front() const { return fthreads.front(); }

fthread_t *fthread_list::pop_front() 
{
  if(fthreads.size() > 0) 
  {
    auto ft = fthreads.front();
    fthreads.pop_front();
    return ft; 
  }
  else return nullptr;
}

void fthread_list::push_front(fthread_t *p) { fthreads.push_front(p); }
void fthread_list::push_back(fthread_t *p) { fthreads.push_back(p); }
size_t fthread_list::size()const { return fthreads.size(); }
::std::list<::flx::rtl::fthread_t*>::iterator fthread_list::begin() { return fthreads.begin(); }
::std::list<::flx::rtl::fthread_t*>::iterator fthread_list::end() { return fthreads.end(); }
fthread_list::~fthread_list () { delete async; }

// ***************************************************
// sync_sched
// ***************************************************
char const *sync_sched::get_fstate_desc(fstate_t fs)
{
  switch(fs)
  {
    case blocked: return "blocked";
    case delegated: return "delegated";
    default: return "Illegal fstate_t";
  }
}

char const *sync_sched::get_fpc_desc()
{
  if (ft)
    return "Next request pos";
  else
  {
    if (active->size() > 0) return "Next fthread pos";
    else return "Out of active threads";
  }
}


sync_sched::sync_sched (
  bool debug_driver_,
  ::flx::gc::generic::gc_profile_t *gcp_,
  fthread_list *active_
) :
  debug_driver(debug_driver_),
  collector(gcp_->collector),
  active(active_),
  ft(nullptr)
{}


void sync_sched::show_state () {
    if (debug_driver)
      fprintf(stderr, "CUR[%p] ACT[%p]\n",ft,
        active->size()?active->front():NULL);
  }

// so it is made a root first
void sync_sched::push_new(fthread_t *f) {
  muxguard dummy(active->pactive_lock);
  impl_push_new (f);
}

// used by async to activate fthread in ready (async complete) queue
void sync_sched::push_old(fthread_t *f) {
  muxguard dummy(active->pactive_lock);
  impl_push_old (f);
}
void sync_sched::impl_push_old(fthread_t *f) 
  {
    if(ft) active->push_front(ft);
    ft = f;
  }

void sync_sched::impl_push_new(fthread_t *f)
  {
    if (debug_driver)
      fprintf(stderr,"[sync] pthread %p adding(new root) fthread %p\n",::flx::pthread::mythrid(),ft);
    collector->add_root(f);
    impl_push_old(f);
  }

void sync_sched::do_yield()
    {
      if(debug_driver)
         fprintf(stderr,"[sync: svc_yield] yield");
      
      muxguard dummy(active->pactive_lock);
      active->push_back(ft);
      ft = active->pop_front();
    }

void sync_sched::do_spawn_fthread()
    {
      muxguard dummy(active->pactive_lock);
      fthread_t *ftx = *(fthread_t**)request->data;
      if(debug_driver)
        fprintf(stderr,"[sync: svc_spawn_fthread] Spawn fthread %p\n",ftx);
      impl_push_new(ftx);
    }

void sync_sched::do_schedule_fthread()
    {
      muxguard dummy(active->pactive_lock);
      fthread_t *ftx = *(fthread_t**)request->data;
      if(debug_driver)
        fprintf(stderr,"[sync: svc_schedule_fthread] Schedule fthread %p\n",ftx);
      collector->add_root(ftx);
      active->push_back(ftx);
    }

void sync_sched::do_sread()
    {
      muxguard dummy(active->pactive_lock);
      readreq_t * pr = (readreq_t*)request->data;
      schannel_t *chan = pr->chan;
      if(debug_driver)
        fprintf(stderr,"[sync: svc_read] Fibre %p Request to read on channel %p\n",ft,chan);
      if(chan==NULL) goto svc_read_none;
    svc_read_next:
      {
        fthread_t *writer= chan->pop_writer();
        if(writer == 0) goto svc_read_none;       // no writers
        if(writer->cc == 0) goto svc_read_next;   // killed
        readreq_t * pw = (readreq_t*)writer->get_svc()->data;
        if(debug_driver)
          fprintf(stderr,"[sync: svc_read] Writer @%p=%p, read into %p\n", 
            pw->variable,*(void**)pw->variable, pr->variable);
        if (pr->variable && pw->variable)
          *(void**)pr->variable = *(void**)pw->variable;
        if(debug_driver)
          fprintf(stderr,"[sync: svc_read] current fibre %p FED, fibre %p UNBLOCKED\n",ft, writer);

        // WE are the reader, stay current, push writer
        // onto active list
        active->push_front(writer);
        collector->add_root(writer);
show_state();
        return;
      }

    svc_read_none:
      if(debug_driver)
        fprintf(stderr,"[sync: svc_read] No writers on channel %p: fibre %p HUNGRY\n",chan,ft);
      chan->push_reader(ft);
      if(ft) 
      {
         collector->remove_root(ft);
         ft = active->pop_front();
      }
show_state();
      return;
    }

void sync_sched::do_swrite()
    {
      muxguard dummy(active->pactive_lock);
      readreq_t * pw = (readreq_t*)request->data;
      schannel_t *chan = pw->chan;
      if(debug_driver)
         fprintf(stderr,"[sync: svc_write] Fibre %p Request to write on channel %p\n",ft,chan);
      if(chan==NULL)goto svc_write_none;
    svc_write_next:
      {
        fthread_t *reader= chan->pop_reader();
        if(reader == 0) goto svc_write_none;     // no readers
        if(reader->cc == 0) goto svc_write_next; // killed
        readreq_t * pr = (readreq_t*)reader->get_svc()->data;
        if(debug_driver)
          fprintf(stderr,"[sync: svc_write] Writer @%p=%p, read into %p\n", 
            pw->variable,*(void**)pw->variable, pr->variable);
        if (pr->variable && pw->variable)
          *(void**)pr->variable = *(void**)pw->variable;
        if(debug_driver)
          fprintf(stderr,"[sync: svc_write] hungry fibre %p FED\n",reader);

        // WE are the writer, push us onto the active list
        // and make the reader on the channel current
        impl_push_new (reader);
show_state();
        return;
      }
    svc_write_none:
      if(debug_driver)
        fprintf(stderr,"[sync: svc_write] No readers on channel %p: fibre %p BLOCKING\n",chan,ft);
      chan->push_writer(ft);
      if(ft) 
      {
         collector->remove_root(ft);
         ft = active->pop_front();
      }
show_state();
      return;
    }

// NOTE: not protected by mutex
void sync_sched::external_multi_swrite (schannel_t *chan, void *data)
    {
      if(chan==NULL) return;
    svc_multi_write_next:
      fthread_t *reader= chan->pop_reader();
      if(reader == 0)  return;    // no readers left
      if(reader->cc == 0) goto svc_multi_write_next; // killed
      {
        readreq_t * pr = (readreq_t*)reader->get_svc()->data;
        if(debug_driver)
           fprintf(stderr,"[sync: svc_multi_write] Write data %p, read into %p\n", 
             data, pr->variable);
        if (pr->variable)
          *(void**)pr->variable = data;
        impl_push_new(reader);
      }
      goto svc_multi_write_next;
    }

void sync_sched::do_multi_swrite()
    {
      muxguard dummy(active->pactive_lock);
      readreq_t * pw = (readreq_t*)request->data;
      void *data = *(void**)pw->variable;
      schannel_t *chan = pw->chan;
      if(debug_driver)
        fprintf(stderr,"[sync: svc_multi_write] Request to write on channel %p\n",chan);
      external_multi_swrite (chan, data);
    }

void sync_sched::do_kill()
    {
      muxguard dummy(active->pactive_lock);
      fthread_t *ftx = *(fthread_t**)request->data;
      if(debug_driver)fprintf(stderr,"[sync: svc_kill] Request to kill fthread %p\n",ftx);
      ftx -> kill();
      return;
    }


// NOTE: the currently running fibre variable is owned
// by this sync scheduler and is not shared, so access to
// it does not required serialisation

sync_sched::fstate_t sync_sched::frun()
{
  if (debug_driver)
     fprintf(stderr,"[sync] frun: entry ft=%p, active size=%zu\n", ft,active->size());
dispatch:
  if (ft == 0) {
     muxguard dummy(active->pactive_lock);
     ft = active->pop_front(); 
     if (debug_driver)
       fprintf(stderr,"[sync] pthread %p fetching fthread %p\n",::flx::pthread::mythrid(),ft);
   }
  if (ft == 0) return blocked; 
  request = ft->run();        // run fthread to get request
  if(request == 0)            // euthenasia request
  {
    muxguard dummy(active->pactive_lock);
    if(debug_driver)
      fprintf(stderr,"[sync] pthread %p unrooting fthread %p\n",::flx::pthread::mythrid(),ft);
    collector->remove_root(ft);
    ft = 0;
    goto dispatch;
  }

  if (debug_driver)
    fprintf(stderr,"[flx_sync:sync_sched] dispatching service request %d\n", request->variant);
  switch(request->variant)
  {
    case svc_yield: do_yield(); goto dispatch;

    case svc_spawn_fthread : do_spawn_fthread(); goto dispatch;
    case svc_schedule_fthread: do_schedule_fthread(); goto dispatch;

    case svc_sread: do_sread(); goto dispatch;

    case svc_swrite: do_swrite(); goto dispatch;

    case svc_multi_swrite: do_multi_swrite(); goto dispatch;

    case svc_kill: do_kill(); goto dispatch;

    default:  
      return delegated;
  }
}

}}
@

